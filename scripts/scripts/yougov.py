import os
import sys
import pytz
import json
import datetime
import requests

from tqdm import tqdm
import numpy as np
import pandas as pd


DEBUG = False

# MIN_RESPONSES: country-date-question observations with less than this
# many valid responses will be dropped. If "None", no observations will
# be dropped.
MIN_RESPONSES = 500

# FREQ: temporal level at which to aggregate the individual survey
# responses, passed as the `freq` argument to
# pandas.Series.dt.to_period. Must conform to a valid Pandas offset
# string (e.g. 'M' = "month", "W" = "week").
FREQ = 'M'

# ZERO_DAY: reference date for internal yearIsDay Grapher usage.
ZERO_DAY = "2020-01-21"

# File paths
CURRENT_DIR = os.path.dirname(__file__)
sys.path.append(CURRENT_DIR)
INPUT_PATH = os.path.join(CURRENT_DIR, "../input/yougov")
OUTPUT_PATH = os.path.join(CURRENT_DIR, "../grapher")
MAPPING_PATH = os.path.join(INPUT_PATH, "mapping.csv")
MAPPING_VALUES_PATH = os.path.join(INPUT_PATH, 'mapped_values.json')

MAPPING = pd.read_csv(MAPPING_PATH, na_values=None)
MAPPING['label'] = MAPPING['label'].str.lower()
with open(MAPPING_VALUES_PATH, 'r') as f:
    MAPPED_VALUES = json.load(f)


class YouGov:

    def __init__(self,  output_path: str, debug: bool = False):
        self.source_url = "https://github.com/YouGov-Data/covid-19-tracker/raw/master"
        self.debug = debug
        self.output_path = output_path
        self.dataset_name = "YouGov-Imperial COVID-19 Behavior Tracker"

    @property
    def output_csv_path(self):
        return os.path.join(self.output_path, f"{self.dataset_name}.csv")

    @property
    def output_csv_path_composite(self):
        return os.path.join(self.output_path, f"{self.dataset_name}, composite variables.csv")

    def _get_source_url_country(self, country, extension):
        return f"{self.source_url}/data/{country}.{extension}"

    @property
    def source_url_master(self):
        return f"{self.source_url}/countries.csv"

    @property
    def list_countries(self):
        """Get list of countries to download."""
        # Get list of countries
        countries = list(pd.read_csv(
            self.source_url_master,
            header=None)[0]
        )
        if self.debug:
            return countries[:3]
        return countries

    def read(self):
        """Read data. Reads multiple countries and concatenates them into one file."""
        # Load countries
        all_data = []
        for country in tqdm(self.list_countries):
            tqdm.write(country)
            df = self.read_country(country)
            all_data.append(df)
        # Build DataFrame
        df = pd.concat(all_data, axis=0)
        if df.columns.nunique() != df.columns.shape[0]:
            raise ValueError("There are one or more duplicate columns, which may cause unexpected errors.")
        return df

    def read_country(self, country):
        """Read individual country data."""
        # Load df from web
        extensions = ["csv", "zip"]
        df = None
        for ext in extensions:
            url = self._get_source_url_country(country, ext)
            if requests.get(url).ok:
                df = self._read_country_from_web(url)
        if df is None:
            raise ValueError(f"No file found for {country}")
        # Parse date field
        df = df.assign(country=country)
        df.columns = df.columns.str.lower()
        return df

    def _read_country_from_web(self, source_url_country):
        """Given URL, reads individual country data."""
        return pd.read_csv(
            source_url_country,
            low_memory=False,
            na_values=[
                "", "Not sure", " ", "Prefer not to say", "Don't know", 98, "Don't Know",
                "Not applicable - I have already contracted Coronavirus (COVID-19)",
                "Not applicable - I have already contracted Coronavirus"
            ]
        )

    def pipeline_csv(self, df: pd.DataFrame):
        df = (
            df
            .pipe(_format_date)
            .pipe(_subset_and_rename_columns)
            .pipe(_preprocess_cols)
            .pipe(_derive_cols)
            .pipe(_standardize_entities)
            .pipe(_aggregate)
        )
        df_comp = _create_composite_cols(df)
        if df_comp is not None:
            df_comp = df_comp.pipe(_rename_columns).pipe(_reorder_columns)
        df = (
            df
            .pipe(_round)
            .pipe(_rename_columns)
            .pipe(_reorder_columns)
        )
        return df, df_comp

    def to_db(self):
        from utils.db_imports import import_dataset

        time_str = datetime.datetime.now().astimezone(pytz.timezone('Europe/London')).strftime("%-d %B %Y, %H:%M")
        source_name = (
            f"Imperial College London YouGov Covid 19 Behaviour Tracker Data Hub â€“ Last updated {time_str} "
            f"(London time)"
        )
        import_dataset(
            dataset_name=self.dataset_name,
            namespace='owid',
            csv_path=self.output_csv_path,
            default_variable_display={
                'yearIsDay': True,
                'zeroDay': ZERO_DAY
            },
            source_name=source_name,
            slack_notifications=False
        )

    def to_csv(self):
        df = self.read()
        df, df_comp = df.pipe(self.pipeline_csv)

        # Export
        if df_comp is not None:
            df_comp.to_csv(
                self.output_csv_path_composite, 
                index=False
            )
        df.to_csv(self.output_csv_path, index=False)


def _format_date(df: pd.DataFrame):
        df.loc[:, "date"] = pd.to_datetime(df.endtime, format="%d/%m/%Y %H:%M", errors="coerce")
        mask = df.date.isnull()
        df.loc[mask, 'date'] = pd.to_datetime(df[mask]['date'], format="%Y-%m-%d %H:%M:%S", errors='coerce')
        return df


def _subset_and_rename_columns(df):
    """keeps only the survey questions with keep=True in mapping.csv and
    renames columns.
    Note: we do not use `df.rename(columns={...})` because for some columns we 
        derive multiple variables.
    """
    assert MAPPING.keep.isin([True, False]).all(), 'All values in "keep" column of `MAPPING` must be True or False.'
    assert MAPPING['code_name'].duplicated().sum() == 0, (
        "All rows in the `code_name` field of mapping.csv must be unique."
    )
    index_cols = ['country', 'date']
    df2 = df[index_cols]
    for row in MAPPING[MAPPING.keep & ~MAPPING.derived].itertuples():
        df2.loc[:, row.code_name] = df[row.label]
    return df2


def _preprocess_cols(df):
    for row in MAPPING[MAPPING.preprocess.notnull()].itertuples():
        if row.code_name in df.columns:
            df.loc[:, row.code_name] = df[row.code_name].replace(MAPPED_VALUES[row.preprocess])
            uniq_values = set(MAPPED_VALUES[row.preprocess].values())
            assert df.loc[:, row.code_name].drop_duplicates().dropna().isin(uniq_values).all(), f"One or more non-NaN values in {row.code_name} are not in {uniq_values}"
    return df


def _derive_cols(df):
    derived_variables_to_keep = MAPPING[MAPPING['derived'] & MAPPING['keep']].code_name.unique().tolist()
    if 'covid_vaccinated_or_willing' in derived_variables_to_keep:
        # constructs the covid_vaccinated_or_willing variable
        # pd.crosstab(df['vac'].fillna(-1), df['vac_1'].fillna(-1))
        vac_min_val = min(
            MAPPED_VALUES[MAPPING.loc[
                MAPPING['code_name'] == 'covid_vaccine_received_one_or_two_doses',
                'preprocess'
            ].squeeze()].values()
        )
        vac_max_val = max(
            MAPPED_VALUES[MAPPING.loc[
                MAPPING['code_name'] == 'covid_vaccine_received_one_or_two_doses',
                'preprocess'
            ].squeeze()].values()
        )
        vac_1_max_val = max(
            MAPPED_VALUES[MAPPING.loc[
                    MAPPING['code_name'] == 'willingness_covid_vaccinate_this_week',
                    'preprocess'
            ].squeeze()].values()
        )

        assert not ((df['covid_vaccine_received_one_or_two_doses'] == vac_max_val) & df['willingness_covid_vaccinate_this_week'].notnull()).any(), (
            "Expected all vaccinated respondents to NOT be asked whether they would "
            "get vaccinated, but found at least one vaccinated respondent who was "
            "asked the latter question."
        )
        assert not ((df['covid_vaccine_received_one_or_two_doses'] == vac_min_val) & df['willingness_covid_vaccinate_this_week'].isnull()).any(), (
            "Expected all unvaccinated respondents to be asked whether they would "
            "get vaccinated, but found at least one unvaccinated respondent who was "
            "not asked the latter question."
        )

        df.loc[:, 'covid_vaccinated_or_willing'] = (
            (df['covid_vaccine_received_one_or_two_doses'] == vac_max_val) | 
            (df['willingness_covid_vaccinate_this_week'] == vac_1_max_val)
        ).astype(int) * vac_max_val
        df.loc[df['covid_vaccine_received_one_or_two_doses'].isnull() & df['willingness_covid_vaccinate_this_week'].isnull(), 'covid_vaccinated_or_willing'] = np.nan

    return df


def _standardize_entities(df):
    df.loc[:, "entity"] = df.country.apply(lambda x: x.replace("-", " ").title())
    df = df.drop(columns=["country"])
    return df


def _aggregate(df):
    s_period = df["date"].dt.to_period(FREQ)
    df.loc[:, "date_end"] = s_period.dt.end_time.dt.date
    today = datetime.datetime.utcnow().date()
    if df['date_end'].max() > today:
        df.loc[:, "date_end"] = df['date_end'].replace({df['date_end'].max(): today})
    
    questions = [q for q in MAPPING.code_name.tolist() if q in df.columns]

    # computes the mean for each country-date-question observation
    # (returned in long format)
    df_means = (
        df
        .groupby(["entity", "date_end"])[questions]
        .mean()
        .rename_axis('question', axis=1)
        .stack()
        .rename('mean')
        .to_frame()
    )
    
    # counts the number of non-NaN responses for each country-date-question
    # observation (returned in long format)
    df_counts = (
        df.groupby(["entity", "date_end"])[questions]
        .apply(lambda gp: gp.notnull().sum())
        .rename_axis('question', axis=1)
        .stack()
        .rename('num_responses')
        .to_frame()
    )
    
    df_agg = pd.merge(df_means, df_counts, left_index=True, right_index=True, how='outer', validate='1:1')
    
    if MIN_RESPONSES:
        df_agg = df_agg[df_agg['num_responses'] >= MIN_RESPONSES]
    
    # converts dataframe back to wide format.
    df_agg = df_agg.unstack().reset_index()
    new_columns = []
    for lvl0, lvl1 in df_agg.columns:
        if lvl1:
            if lvl0 == 'num_responses':
                col = f'{lvl1}__{lvl0}'
            else:
                col = lvl1
        else:
            col = lvl0
        new_columns.append(col)
    df_agg.columns = new_columns
    df_agg.rename(columns={'date_end': 'date'}, inplace=True)

    # constructs date variable for internal Grapher usage.
    df_agg.loc[:, "date_internal_use"] = (df_agg['date'] - datetime.datetime.strptime(ZERO_DAY, '%Y-%m-%d')).dt.days
    df_agg.drop('date', axis=1, inplace=True)

    return df_agg


def _create_composite_cols(df):
    ffill_limit = 7
    vac_var_id = 145610
    try:
        res = requests.get(f"https://ourworldindata.org/grapher/data/variables/{vac_var_id}.json")
        assert res.ok
        vac_data = json.loads(res.content)
        var_name = vac_data['variables'][f'{vac_var_id}']['name']
        assert ZERO_DAY == vac_data['variables'][f'{vac_var_id}']['display']['zeroDay'], (
            "Zero days do not match. Data merge will not be correct."
        )
        df_vac = pd.DataFrame({
            'date': vac_data['variables'][f'{vac_var_id}']['years'],
            'entity': [vac_data['entityKey'][f'{ent}']['name'] for ent in vac_data['variables'][f'{vac_var_id}']['entities']],
            var_name: vac_data['variables'][f'{vac_var_id}']['values'],
        }).sort_values(['entity', 'date'], ascending=True)
        date_range = list(range(df_vac['date'].min(), df_vac['date'].max() + 1))
        df_vac[df_vac['entity'] == 'United States'].set_index('date').reindex(date_range)
        df_vac = df_vac.groupby('entity').apply(lambda gp: gp.set_index('date').reindex(date_range)).drop('entity', axis=1).reset_index().sort_values(['entity', 'date'])
        df_vac[var_name] = df_vac.groupby('entity')[var_name].apply(lambda gp: gp.ffill(limit=ffill_limit)).dropna()
        df_vac.dropna(subset=[var_name], inplace=True)

        vac_entities = df_vac['entity'].unique()
        yougov_entities_not_found = [ent for ent in df['entity'].drop_duplicates() if ent not in vac_entities]
        assert len(yougov_entities_not_found) < (df['entity'].drop_duplicates().shape[0] * 0.1), (
            "Expected nearly all YouGov entities to be in vaccination data, but "
            "failed to find >10% of YouGov entities in the vaccination data. "
            f"Entities not found: {yougov_entities_not_found}"
        )
        
        df_temp = pd.merge(
            df[[
                'entity', 
                'date_internal_use', 
                'willingness_covid_vaccinate_this_week', 
                'unwillingness_covid_vaccinate_this_week',
                'uncertain_covid_vaccinate_this_week'
            ]], 
            df_vac[[
                'entity',
                'date',
                var_name
            ]], 
            left_on=['entity', 'date_internal_use'], 
            right_on=['entity', 'date'], 
            how='inner',
            validate='1:1',
        )
        df_temp[var_name] = df_temp[var_name].round(2)

        # converts willingness to get vaccinated variables to a percentage of the
        # overall population, instead of percentage of the unvaccinated population.
        df_temp['willingness_covid_vaccinate_this_week_pct_pop'] = (
            (100 - df_temp[var_name]) * (df_temp['willingness_covid_vaccinate_this_week']/100)
        ).round(2)
        df_temp['unwillingness_covid_vaccinate_this_week_pct_pop'] = (
            (100 - df_temp[var_name]) * (df_temp['unwillingness_covid_vaccinate_this_week']/100)
        ).round(2)
        df_temp['uncertain_covid_vaccinate_this_week_pct_pop'] = (
            (100 - df_temp[var_name]) * (df_temp['uncertain_covid_vaccinate_this_week']/100)
        ).round(2)

        cols = [
            var_name,
            'willingness_covid_vaccinate_this_week_pct_pop',
            'unwillingness_covid_vaccinate_this_week_pct_pop',
            'uncertain_covid_vaccinate_this_week_pct_pop'
        ]
        df_temp.sample(10)
        assert all(df_temp[cols].sum(axis=1, min_count=len(cols)).dropna().round(1) == 100), (
            f"Expected {cols} to sum to *nearly* 100 for every entity-date "
             "observation, prior to rounding adjustment."
        )
        
        # adjusts one variable to ensure sum of all cols equals exactly
        # 100. Otherwise, rounding errors may lead the sum to be
        # slightly off (e.g. 99.99).
        df_temp[f'{cols[-1]}_adjusted'] = (100 - df_temp[cols[:-1]].sum(axis=1)).round(2)
        assert all((df_temp[cols[-1]] - df_temp[f'{cols[-1]}_adjusted']).abs() < 0.1), (
            f"Expected rounding adjustment of {cols[-1]} to be minor (< 0.1), "
             "but adjustment was larger than this for one or more entity-date "
             "observations."
        )
        assert all(df_temp[cols[:-1] + [f'{cols[-1]}_adjusted']].sum(axis=1, min_count=len(cols)).dropna().round(2) == 100), (
            f"Expected {cols} to sum to exactly 100 for every entity-date "
             "observation, after rounding adjustment."
        )
        df_temp[cols[-1]] = df_temp[f'{cols[-1]}_adjusted']
        
        df_temp = df_temp[[
            'entity', 
            'date_internal_use',
            var_name,
            'willingness_covid_vaccinate_this_week_pct_pop',
            'unwillingness_covid_vaccinate_this_week_pct_pop',
            'uncertain_covid_vaccinate_this_week_pct_pop'
        ]]

    except Exception as e:
        df_temp = None
        print(f'Failed to construct composite variables. Error: {e}')

    return df_temp


def _round(df):
    index_cols = ['entity', 'date_internal_use']
    df = df.set_index(index_cols).round(1).reset_index()
    return df


def _rename_columns(df):
    # renames index columns for use in `update_db`.
    df.rename(columns={'entity': 'Country', 'date_internal_use': 'Year'}, inplace=True)
    return df


def _reorder_columns(df):
    index_cols = ['Country', 'Year']
    data_cols = sorted([col for col in df.columns if col not in index_cols])
    df = df[index_cols + data_cols]
    return df


def update_db():
    YouGov(output_path=OUTPUT_PATH, debug=DEBUG).to_db()


def main():
    YouGov(
        output_path=OUTPUT_PATH,
        debug=True
    ).to_csv()


if __name__ == "__main__":
    main()
